:PROPERTIES:
:ID:       39d30d24-c374-4d0c-8037-b03ecbf983fa
:ROAM_ALIASES: VITS
:END:
#+title: computer_vision
#+filetags: :nawanomicon:
- with image detector, image tokenizer https://github.com/Vision-CAIR/MiniGPT-4
  - llava: https://llava-vl.github.io/ https://arxiv.org/abs/2304.08485
- [[https://github.com/salesforce/LAVIS/tree/main/projects/instructblip][InstructBLIP]]: [[http://arxiv.org/abs/2305.06500][Towards General-purpose]] Vision-Language Models with Instruction Tuning
  - image understanding
- LongNet: Scaling Transformers to 1,000,000,000 Tokens
- [[https://twitter.com/_akhaliq/status/1674237851536334849][Towards Language]] Models That Can See: Computer Vision Through the LENS of Natural Language
  - reasoning over independent vision modules
- [[https://twitter.com/_akhaliq/status/1676813163080175616][MSViT]]: Dynamic Mixed-Scale Tokenization for Vision Transformers
  - dynamic tokenizer for ViTs, where the scale at which an image is processed varies based on semantic details
** CAPTIONING
- [[https://twitter.com/_akhaliq/status/1679308968521261056][SITTA]]: A Semantic Image-Text Alignment for Image Captioning
  - linear semantic mappings = image captioning without access to gradient information; less computation
** CLASSIFICATION
- [[https://twitter.com/_akhaliq/status/1668828834181836800][GeneCIS]]: A Benchmark for General Conditional Image Similarity
  - models, should adapt to notion of similarity dynamically
- [[https://twitter.com/_akhaliq/status/1665736170100097024][Vocabulary-free]] Image Classification
- [[https://twitter.com/_akhaliq/status/1666262910081875970][DIffusion FeaTures (DIFT)]]: Emergent Correspondence from Image Diffusion [[ATLAS]] <<diffusion features>>
  - [[https://diffusion-classifier.github.io/][Your Diffusion]] [[https://arxiv.org/abs/2303.16203][Model is]] Secretly a Zero-Shot Classifier
* AUDIO VISION
:PROPERTIES:
:ID:       f03ccf94-1aa5-4705-89af-617a22570e26
:END:
- [[https://github.com/microsoft/muzic/tree/main/clamp][Clamp]]: clip for music
  - [[https://huggingface.co/docs/transformers/model_doc/clap][CLAP]] (Contrastive Language-Audio Pretraining)
- [[https://arxiv.org/pdf/2305.11834.pdf][Pengi]]: An Audio Language Model for Audio Tasks
  - audio understanding
** WHISPER
:PROPERTIES:
:ID:       e54caacc-519a-4187-bafc-4d32c33f1e2b
:END:
- [[https://github.com/Vaibhavs10/translate-with-whisper][whisper]] [[https://twitter.com/reach_vb/status/1673363113888948224][translator]] fast
  - [[https://twitter.com/_akhaliq/status/1677150590516834305][Whisper-AT]]: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers
    - audio representation is actually not noise-invariant
      - audio tagging model on top, <1% extra computational, a single forward pass
  - [[https://twitter.com/xenovacom/status/1678180605836533762][word-level]] timestamps w/ whisper

:PROPERTIES:
:ID:       c7fe7e79-73d3-4cc7-a673-2c2e259ab5b5
:END:
#+title: stable_diffusion
#+filetags: :nawanomicon:
- parent: [[id:82127d6a-b3bb-40bf-a912-51fa5134dacc][diffusion]]
- To generate icons on a neutral background put a pure black image to img2img with denoising strength of 1
- combining [[https://github.com/huggingface/diffusers/tree/main/examples/community#stable-diffusion-mega][pipelines]], creating [[https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline][pipelines]]
- [[https://github.com/chavinlo/distributed-diffusion/tree/rewrite][distributed-diffusion]] using hivemind (distributed training) vs [[https://github.com/microsoft/DeepSpeed][Deepspeed]]
- block merging (u-net) https://rentry.org/BlockMergeExplained
- hyperparameters with extra network https://www.storminthecastle.com/posts/01_head_poser/ [[https://wandb.ai/johnowhitaker/midu-guidance/reports/Mid-U-Guidance-Fast-Classifier-Guidance-for-Latent-Diffusion-Models--VmlldzozMjg0NzA1][Mid-U Guidance]]
- [[https://arxiv.org/pdf/2305.11846.pdf][Any-to-Any]] Generation via Composable Diffusion (audio, imagen, text)
* GENERATION CONTROL
- block [[https://github.com/hako-mikan/sd-webui-lora-block-weight#%E6%A6%82%E8%A6%81][weights lora]]
- extra modules(components) to the unet: [[MCM]] [[ViCo]]
** SIDE TOOLS
[[https://github.com/Zuntan03/CharFramework][CharFramework]] [[https://twitter.com/Zuntan03/status/1640240599323541504][explanations]]
[[https://www.engine.study/blog/modding-age-of-empires-ii-with-a-sprite-diffuser/][sprites]] age of empires
real time [[https://www.reddit.com/r/StableDiffusion/comments/12qlg3b/who_needs_photoshop_anyway_ms_paint_sd/][painting]](diffusing while painting): https://github.com/houseofsecrets/SdPaint
[[https://github.com/tomayac/SVGcode][SVGcode]]: [[https://svgco.de/][Convert]] color bitmap images to color SVG vector images, [[https://github.com/GeorgLegato/stable-diffusion-webui-vectorstudio][auto version]] vector graphics
** CONTROL NETWORKS, CONTROLNET
- why controlnet, alternatives https://github.com/lllyasviel/ControlNet/discussions/188
- controlNet (total control of image generation, from doodles to masks)
  - [[https://www.reddit.com/r/StableDiffusion/comments/12m169y/comment/jg90xs9/?utm_source=share&utm_medium=web2x&context=3][controllable networks]]
  - T2I-Adapter (lighter, similar but less, composable), [[https://www.reddit.com/r/StableDiffusion/comments/11v3dgj/comment/jcrag7x/?utm_source=share&utm_medium=web2x&context=3][how color pallete]]
  - lora based https://github.com/HighCWu/ControlLoRA
  - [[https://huggingface.co/georgefen/Face-Landmark-ControlNet][face landmarks]] [[https://github.com/Mukosame/Anime2Sketch][Anime2Sketch]]
  - [[https://ljzycmd.github.io/projects/MasaCtrl/][MasaCtrl]]: [[https://github.com/TencentARC/MasaCtrl][Tuning-free]] Mutual Self-Attention Control for Consistent Image Synthesis and Editing
    - directly modifying the text prompts
      - better than ddim inversion
    - make SD animations more consistent.
- [[https://huggingface.co/papers/2305.11147][UniControl]]: A Unified Diffusion Model for Controllable Visual Generation In the Wild
  - several controlnets in one, contextual understanding
- In-[[https://github.com/Zhendong-Wang/Prompt-Diffusion][Context]] [[https://zhendong-wang.github.io/prompt-diffusion.github.io/][Learning]] Unlocked for Diffusion Models
  - image to hed, depth, segmentation, outline
- VisorGPT: Learning Visual Prior via Generative Pre-Training
  - [[https://huggingface.co/papers/2305.13777][gpt]] that learns to tranform normal prompts into controlnet primitives
** LAYOUT
 - [[https://gligen.github.io/][GLIGEN]]: Open-Set Grounded Text-to-Image Generation (boxes)
   - [[https://twitter.com/_akhaliq/status/1645253639575830530][Training-Free]] Layout Control with Cross-Attention Guidance
   - [[https://arxiv.org/pdf/2304.14573.pdf][SceneGenie]]: Scene Graph Guided Diffusion Models for Image Synthesis
 - [[https://twitter.com/_akhaliq/status/1673539960664911874][Zero-shot]] spatial layout conditioning for text-to-image diffusion models
   - implicit segmentation maps can be extracted from cross-attention layers
 - [[https://twitter.com/_akhaliq/status/1674623306551508993][Generate Anything]] Anywhere in Any Scene <<layout aware>>
   - training guides to focus on object identity, personalized concept with localization controllability
** ONE IMAGE GUIDED
- Imagic
- pfg Prompt free generation: https://github.com/laksjdjf/pfg
  - old one: [[https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/6585][PaintByExample]]
- <<UMM-Diffusion>>, TIUE: [[https://arxiv.org/abs/2303.09319][Unified Multi-Modal]] Latent Diffusion for Joint Subject and Text Conditional Image Generation
  - takes joint texts and images
  - only image-mapping to pseudo word embedding learned, like [[HiPer]]
- Stable Diffusion Reimagine: conditioning the UNet with the image clip embeddings, then training
- [[https://arxiv.org/pdf/2303.13126.pdf][MagicFusion]]: [[https://magicfusion.github.io/][Boosting]] Text-to-Image Generation Performance by Fusing Diffusion Models
  - saliency(concept) inversion, then generate while masking it
- [[https://twitter.com/_akhaliq/status/1644557225103335425][PAIR-Diffusion]]: Object-Level Image Editing with Structure-and-Appearance
- [[https://github.com/drboog/ProFusion][ProFusion]]: Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach
  - promptnet, encoder based inversion: input image(person) gets semantically converted to output style(anime)
- [[suti]] [[custom-edit diffusion]] [[Inserting Anybody]]
*** VARIATIONS
- image variations model (mix images): https://twitter.com/Buntworthy/status/1615302310854381571
  - by versatile diffusion model guy, [[https://www.reddit.com/r/StableDiffusion/comments/10ent88/guy_who_made_the_image_variations_model_is_making/][reddit]]
    - improved: https://github.com/SHI-Labs/Versatile-Diffusion
  - image only semantic alignment https://arxiv.org/pdf/2302.03956.pdf
* BETTER DIFFUSION
- editing [[https://time-diffusion.github.io/TIME_paper.pdf][default]] of a prompt: https://github.com/bahjat-kawar/time-diffusion
- [[https://github.com/SusungHong/Self-Attention-Guidance][Self-Attention Guidance]] (SAG): [[https://arxiv.org/pdf/2210.00939.pdf][SAG leverages]] [[https://github.com/ashen-sensored/sd_webui_SAG][intermediate attention]] maps of diffusion models at each iteration to capture essential information for the generative process and guide it accordingly
  - pretty much just reimplemented the attention function without changing much else,
  - Aligning Text-to-Image Models using Human Feedback https://arxiv.org/abs/2302.12192
    - [[https://tgxs002.github.io/align_sd_web/][Better Aligning]] Text-to-Image Models with Human Preference
    - [[https://github.com/GanjinZero/RRHF][RRHF]]: Rank Responses to Align Language Models with Human Feedback without tears
    - [[https://github.com/THUDM/ImageReward][ImageReward]]: [[https://arxiv.org/abs/2304.05977][Learning]] and Evaluating Human Preferences for Text-to-Image Generation
** STEERING
- [[THROUGH PROMPT]]
- [[Diffusion Self-Guidance for Controllable Image Generation][Diffusion Self-Guidance]] [[https://dave.ml/selfguidance/][for Controllable]] Image Generation
  - steer sampling, similarly to classifier guidance, but using signals in the pretrained model itself
  - instructional transfomations <<Self-Guidance>>
*** CONDITIONAL
- [[https://github.com/thu-ml/unidiffuser][unidiffuser]]: marginal, conditional, and joint diffusion, [[https://ml.cs.tsinghua.edu.cn/diffusion/unidiffuser.pdf][paper]] [[https://arxiv.org/abs/2303.06555][arxiv]] multi-modal data in one model
  - old: Versatile [[https://github.com/SHI-Labs/Versatile-Diffusion][Diffusion]]
- [[https://arxiv.org/abs/2303.09833][FreeDoM]]: [[https://github.com/vvictoryuki/FreeDoM][Training-Free]] Energy-Guided Conditional Diffusion Model <<FreeDoM>>
  - has list of deblurring, super-resolution and restoration methods
*** UNIVERSAL GUIDANCE FOR DIFFUSION MODELS
  - [[https://github.com/arpitbansal297/Universal-Guided-Diffusion][Universal Guided Diffusion]]
  - alternatives:
    - facial recognition (same face)                     <<[[e4t]], lora [[https://github.com/cloneofsimo/lora/discussions/96][masked score estimation]]
    - object recognition - areas                         <<Directed Diffusion, gligen
    - masked (target segmentation map)                   <<controlnet
    - style (polygons, origami, embroidery)              <<plug-and-play, pix2pixzero,
    - pix2pix instructions                               <<sega
    - color pallete (brushes)                            <<MCM
** SD GENERATION OPTIMIZATION
- [[https://twitter.com/Birchlabs/status/1640033271512702977][turning off]] [[https://github.com/Birch-san/diffusers-play/commit/77fa7f965edf7ab7280a47d2f8fc0362d4b135a9][CFG when]] denoising sigmas below 1.1
- Tomesd: [[https://github.com/dbolya/tomesd][Token Merging]] for [[https://arxiv.org/abs/2303.17604][Stable Diffusion]] [[https://git.mmaker.moe/mmaker/sd-webui-tome][code]]
- Nested Diffusion Processes for Anytime Image Generation
  - can generate viable when stopped arbitrarily before completion
- [[https://twitter.com/_akhaliq/status/1668076625924177921][BOOT]]: Data-free Distillation of Denoising Diffusion Models with Bootstrapping
  - use sd as teacher model and train faster one using it as bootstrap; 30 fps
* SAMPLERS
- fastest solver https://arxiv.org/abs/2301.12935
  - another accelerator: https://arxiv.org/abs/2301.11558
- unipc sampler (sampling in 5 steps)
  - [[https://blog.novelai.net/introducing-nai-smea-higher-image-generation-resolutions-9b0034ffdc4b][smea]]: (nai) global attention sampling
- Karras no blurry improvement [[https://www.reddit.com/r/StableDiffusion/comments/11mulj6/quality_improvements_to_dpm_2m_karras_sampling/][reddit]]
* NOISE MANIPULATION
- shifted noise, pyramid noise
- Attend-and-Excite ([[https://attendandexcite.github.io/Attend-and-Excite/][excite]] ignored prompt [[https://github.com/AttendAndExcite/Attend-and-Excite][tokens]]) (no retrain)
  - [[https://arxiv.org/pdf/2302.13153.pdf][Directed Diffusion]]: [[https://github.com/hohonu-vicml/DirectedDiffusion][Direct Control]] of Object Placement through Attention Guidance (no retrain) [[https://github.com/giga-bytes-dev/stable-diffusion-webui-two-shot/tree/ashen-sensored_directed-diffusion][repo]]
  - [[https://mcm-diffusion.github.io/][MCM]] [[https://arxiv.org/pdf/2302.12764.pdf][Modulating Pretrained]] Diffusion Models for Multimodal Image Synthesis (module after denoiser) mmc
    - mask like control to tilt the noise, maybe useful for text <<MCM>>
  - paint with words
- [[https://arxiv.org/abs/2301.11093v1][simple diffusion]]: End-to-end diffusion for high resolution images
  - shifted scheduled noise
* IMAGE EDITING
- [[https://github.com/cloneofsimo/magicmix][magicmix]] merge shapes
  - [[https://arxiv.org/abs/2303.16765][MDP]]: [[https://github.com/QianWangX/MDP-Diffusion][A Generalized]] Framework for Text-Guided Image Editing by Manipulating the Diffusion Path
    - <Our manipulations and baselines>
- IMAGIC (diffusers)
  - HiPer: [[https://arxiv.org/abs/2303.08767][Highly Personalized]] Text Embedding for Image Manipulation by Stable Diffusion
    - a single image and target text, like accient [[https://github.com/7eu7d7/DreamArtist-sd-webui-extension][DreamArtist]]
    - builds up text-embedding which is concatenated <<HiPer>>, [[UMM-Diffusion]]
- null-text inversion (prompttoprompt but cooler) https://arxiv.org/pdf/2211.09794.pdf
  - https://github.com/cccntu/efficient-prompt-to-prompt
  - imagic: editing photo with prompt: https://github.com/ShivamShrirao/diffusers/tree/main/examples/imagic
  - no fine tuning, using BLIP: https://github.com/pix2pixzero/pix2pix-zero <<pix2pix>>
- plug-and-[[https://github.com/MichalGeyer/plug-and-play][play]] (like pix2pix but features extracted)
- [[https://twitter.com/_akhaliq/status/1670677370276028416][MagicBrush]]: A Manually Annotated Dataset for Instruction-Guided Image Editing
- [[Self-Guidance]]
- [[https://arxiv.org/abs/2211.07825][Direct Inversion]]: Optimization-Free Text-Driven Real Image Editing with Diffusion Models
  - [[Ledits]]
** STYLE
- [[https://arxiv.org/abs/2303.15649][StyleDiffusion]]: Prompt-Embedding Inversion for Text-Based Editing
  - preserve the object-like attention maps after editing
- [[https://huggingface.co/papers/2306.00983][StyleDrop]]: [[https://styledrop.github.io/][Text-to-Image]] [[https://github.com/zideliu/StyleDrop-PyTorch][Generation]] in Any Style (muse architecture)
  - 1% of parameters (painting style)
** REGIONS
- different inpainting with diffusers: https://github.com/huggingface/diffusers/pull/1585
- [[https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model][RDM-Region-Aware-Diffusion-Model]] edits only the region of interest
  - [[https://github.com/mkshing/e4t-diffusion][E4T-diffusion]]: [[https://tuning-encoder.github.io/][Tuning]] [[https://arxiv.org/abs/2302.12228][encoder]]: the text embedding + offset weights <<e4t>> (Needs a >40GB GPU )
    - [[https://arxiv.org/pdf/2302.13848.pdf][Elite]] Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation
      - extra neural network to get text embedding, fastest text embeddings
- [[https://delta-denoising-score.github.io/][Delta]] [[https://arxiv.org/abs/2304.07090][Denoising]] Score: minimal modifications, keeping the image
- [[https://huggingface.co/papers/2305.18286][Photoswap]]: Personalized Subject Swapping in Images
*** REGIONS MERGE
- [[https://arxiv.org/abs/2211.15518][ReCo]]: region control, counting donuts
- [[https://zengyu.me/scenec/][SceneComposer]]: paint with words but cooler
  - bounding boxes instead: [[https://github.com/gligen/GLIGEN][GLIGEN]]: image grounding
  - better VAE and better masks: https://lipurple.github.io/Grounded_Diffusion/
    - [[pix2pix]]
- Collage Diffusion https://arxiv.org/pdf/2303.00262.pdf (harmonize collaged images)
  - [[https://research.nvidia.com/labs/dir/diffcollage/][DiffCollage]]: Parallel Generation of Large Content with Diffusion Models
- [[https://github.com/lunarring/latentblending][Latent]] Blending (interpolate latents)
  - latent couple, multidiffusion, [[https://note.com/gcem156/n/nb3d516e376d7][attention couple]]
    - comfy ui like but masks: https://github.com/omerbt/MultiDiffusion
    - [[https://twitter.com/_akhaliq/status/1667033318590672896][SyncDiffusion]]: Coherent Montage via Synchronized Joint Diffusions (synchronizes them) ==best==
** SPECIFIC CONCEPTS
- [[https://dxli94.github.io/BLIP-Diffusion-website/][BLIP-Diffusion]]: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing
  - learned in 40 steps vs Textual Inversion 3000
  - Subject-driven Style Transfer, Subject Interpolation
  - concept replacement
  - [[https://arxiv.org/pdf/2305.15779.pdf][Custom-Edit]]: Text-Guided Image Editing with Customized Diffusion Models <<custom-edit diffusion>>
- ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation
  - extra on top, not finetune the original diffusion model, awesome quality, <<ViCo>>
- [[HiPer]], [[P+]] : learning text embeddings for each layer of the unet
- [[https://ziqihuangg.github.io/projects/reversion.html][ReVersion]]: [[https://github.com/ziqihuangg/ReVersion][Diffusion-Based]] Relation Inversion from Images (textual inversion for verbs) material textures
- [[https://huggingface.co/papers/2306.00926][Inserting Anybody]] in Diffusion Models via Celeb Basis <<Inserting Anybody>>
  - one facial photograph, 1024 learnable parameters, 3 minutes; several at once
- [[https://twitter.com/_akhaliq/status/1668450247385796609][Controlling]] [[https://github.com/Zeju1997/oft][Text-to-Image]] Diffusion by Orthogonal Finetuning
  - preserves the hyperspherical energy of the pairwise neuron relationship, semantic coherance
- [[layout aware]]
*** CONES
- [[https://arxiv.org/abs/2303.05125][Cones]]: [[https://github.com/Johanan528/Cones][Concept Neurons]] [[https://github.com/damo-vilab/Cones][in Diffusion]] Models for Customized Generation (better than Custom Diffusion)
  - index only the locations in the layers that give rise to a subject, add them together to include multiple subjects in a new context
  - [[Cones 2: Customizable Image Synthesis with Multiple Subjects][cones 2]] [[https://twitter.com/__Johanan/status/1664495182379884549][twitter]] [[https://arxiv.org/pdf/2305.19327.pdf][arxiv]]
    - flexible composition of various subjects without any model tuning
    - leaning an extra on top of a regular text embedding, and using layout to compose
*** FINETUNNING-LESS
- [[https://twitter.com/_akhaliq/status/1645254918121422859][InstantBooth]]: Personalized Text-to-Image Generation without Test-Time Finetuning
  - personalized images with only a single forward pass
- [[https://twitter.com/WenhuChen/status/1643079958388940803][SuTi]]: [[https://open-vision-language.github.io/suti/][Subject-driven]] Text-to-Image Generation via Apprenticeship Learning (using examples)
  - replaces subject-specific fine tuning with in-context learning, <<suti>>
- [[https://twitter.com/_akhaliq/status/1673544034193924103][DomainStudio]]: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data
  - keep the relative distances between adapted samples to achieve generation diversity
**** THROUGH PROMPT
- [[STEERING]]
- [[https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion][magic prompt]]: amplifies-improves the prompt
- hard-prompts-made-easy
- [[https://arxiv.org/abs/2304.03119][Zero-shot]] [[https://arxiv.org/pdf/2304.03119.pdf][Generative]] [[https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation][Model]] Adaptation via Image-specific Prompt Learning
- [[https://arxiv.org/pdf/2305.15581.pdf][Unsupervised Semantic]] Correspondence Using Stable Diffusion
  - <<sematic correspondance>> [[semantic atlas]]; by optimizing prompt, no training
  - find locations in multiple images that have the same semantic meaning
*** SEVERAL CONCEPTS
- [[https://rich-text-to-image.github.io/][Expressive Text-to-Image]] [[https://github.com/SongweiGe/rich-text-to-image][Generation with]] Rich Text (learn concept-map from maxed avarages)
- [[https://arxiv.org/abs/2304.06027][Continual]] [[https://jamessealesmith.github.io/continual-diffusion/][Diffusion]]: Continual Customization of Text-to-Image Diffusion with C-LoRA
  - sequentially learned concepts
- [[https://huggingface.co/papers/2305.16311][Break-A-Scene]]: Extracting Multiple Concepts from a Single Image
- [[https://twitter.com/_akhaliq/status/1653620239735595010][Key-Locked]] Rank One Editing for Text-to-Image Personalization
  - combine individually learned concepts into a single generated image
- [[https://huggingface.co/papers/2305.18292][Mix-of-Show]]: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models
  - solving concept conflicts
*** SVDIFF
- SVDiff: [[https://arxiv.org/pdf/2303.11305.pdf][Compact Parameter]] [[https://arxiv.org/abs/2303.11305][Space]] for Diffusion Fine-Tuning, [[https://twitter.com/mk1stats/status/1643992102853038080][code]]([[https://twitter.com/mk1stats/status/1644830152118120448][soon]])
  - multisubject learning, like D3S
  - personalized concepts, combinable; training gan out of its conv
  - Singular Value Decomposition (SVD) = gene coefficient vs expression level
  - CoSINE: Compact parameter space for SINgle image Editing (remove from prompt after finetune it)
  - [[https://arxiv.org/abs/2304.06648][DiffFit]]: [[https://github.com/mkshing/DiffFit-pytorch][Unlocking]] Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning
    - its PEFT for diffusion
*** ORIGINAL ONES
**** LORA
- lora, lycoris
- use regularization images with lora https://rentry.org/59xed3#regularization-images
- [[https://twitter.com/_akhaliq/status/1668828166499041281][GLORA]]: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
  - individual adapter of each layer,
  - superior accuracy fewer parameters-computations
**** TEXTUAL INVERSION
- [[https://t.co/DbEPmPZB1l][Multiresolution Textual]] [[https://github.com/giannisdaras/multires_textual_inversion][Inversion]]: better textual inversion (embedding)
- Extended Textual Inversion (XTI)
  - [[https://prompt-plus.github.io/][P+]]: [[https://prompt-plus.github.io/files/PromptPlus.pdf][Extended Textual]] Conditioning in Text-to-Image Generation <<P+>>
    - different text embedding per unet layer
    - [[https://github.com/cloneofsimo/promptplusplus][code]]
  - [[https://arxiv.org/abs/2305.05189][SUR-adapter]]: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models (llm)
    - adapter to transfer the semantic understanding of llm to align complex vs simple prompts
** INSTRUCTIONS
- InstructPix2Pix https://www.timothybrooks.com/instruct-pix2pix
  - https://github.com/timothybrooks/instruct-pix2pix
  - https://arxiv.org/abs/2211.09800
- [[https://huggingface.co/spaces/xdecoder/Instruct-X-Decoder][X-Decoder]]: instructPix2Pix [[https://github.com/microsoft/X-Decoder][per]] region(objects)
  - compaable to [[vpd]] <<x-decoder>>
  - [[https://arxiv.org/pdf/2303.17546.pdf][PAIR-Diffusion]]: [[https://github.com/Picsart-AI-Research/PAIR-Diffusion][Object-Level]] Image Editing with Structure-and-Appearance Paired Diffusion Models (region editing)
- pix2pix-zero (promp2prompt without prompt)
  - [[https://github.com/ethansmith2000/MegaEdit][MegaEdit]]: like instructPix2Pix but for any model
    - based on [[EDICT]] and plug-adn-play but using DDIM
      - [[https://twitter.com/SFResearch/status/1612886999152857088][EDICT]]: [[https://github.com/salesforce/EDICT][repo]] Exact Diffusion Inversion via Coupled Transformations
        - like [[sega]] <<EDICT>>, edits-changes object types(dog breeds), like DDIM inversion(ip2p)
* PROMPT CORRECTNESS
- Attend-and-Excite
  - [[https://arxiv.org/abs/2304.03869][Harnessing]] the [[https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn][Spatial-Temporal]] Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis
  - [[https://twitter.com/_akhaliq/status/1670190734543134720][Linguistic]] Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment
    - using prompt sentence structure during inference to improve the faithfulness
- [[https://github.com/hnmr293/sd-webui-cutoff][sd-webui-cutoff]], hide tokens for each separated group, limits the token influence scope (color control)
- simple, [[https://github.com/weixi-feng/Structured-Diffusion-Guidance][Structured Diffusion Guidance]], clip enforces on U-net
  - https://arxiv.org/abs/2212.05032
  - [[https://weixi-feng.github.io/structure-diffusion-guidance/][Training-Free Structured]] Diffusion Guidance for Compositional [[https://arxiv.org/pdf/2212.05032.pdf][Text-to-Image Synthesis]]
  - exploiting language sentences semantical hierarchies (lojban)
** SEMANTIC GUIDANCE
- [[https://github.com/ml-research/semantic-image-editing][sega]] semantic guidance <<sega>> like [[EDICT]]
  - [[https://twitter.com/_akhaliq/status/1664485230151884800][The Hidden]] [[https://huggingface.co/papers/2306.00966][Language]] of Diffusion Models
    - learning interpretable pseudotokens from interpolating unet concepts
    - useful for: single-image decomposition to tokens, bias detection, and semantic image manipulation
- [[https://twitter.com/_akhaliq/status/1676071757994680321][LEDITS]]: Real Image Editing with DDPM Inversion and Semantic Guidance
  - prompt changing, minimal variations <<ledits>>
* USE CASES
** DIFFUSING TEXT
- [[https://ds-fusion.github.io/static/pdf/dsfusion.pdf][DS-Fusion]]: [[https://ds-fusion.github.io/][Artistic]] Typography via Discriminated and Stylized Diffusion (fonts)
- [[https://1073521013.github.io/glyph-draw.github.io/][GlyphDraw]]: [[https://arxiv.org/pdf/2303.17870.pdf][Learning]] [[https://twitter.com/_akhaliq/status/1642696550529867779][to Draw]] Chinese Characters in Image Synthesis Models Coherently
  - [[https://arxiv.org/pdf/2305.10855.pdf][TextDiffuser]]: Diffusion Models as Text Painters
  - [[https://huggingface.co/papers/2305.18259][GlyphControl]]: [[https://github.com/AIGText/GlyphControl-release][Glyph Conditional]] Control for Visual Text Generation ==this==
- [[https://github.com/microsoft/unilm/tree/master/textdiffuser][TextDiffuser]]: [[https://arxiv.org/pdf/2305.10855.pdf][Diffusion]] [[https://huggingface.co/spaces/microsoft/TextDiffuser][Models]] as Text Painters
** IMAGE RESTORATION, SUPER-RESOLUTION
- [[FreeDoM]]
- [[https://arxiv.org/abs/2304.08291][refusion]]: Image Restoration with Mean-Reverting Stochastic Differential Equations
- image restoration IR https://arxiv.org/pdf/2212.00490.pdf
  - using NULL-SPACE
  - https://github.com/wyhuai/DDNM
  - unlitmited superresolution https://arxiv.org/pdf/2303.00354.pdf
- [[https://twitter.com/_akhaliq/status/1674249594421608448][SVNR]]: Spatially-variant Noise Removal with Denoising Diffusion
  - real life noise fixing
** DEPTH GENERATION
- [[https://twitter.com/_akhaliq/status/1630747135909015552][depth map]] from diffusion, build 3d enviroment with it
  - [[https://github.com/wl-zhao/VPD][VPD]]: using diffusion for depth estimation, image segmentation (better) <<vpd>> comparable [[x-decoder]]
- [[https://github.com/isl-org/ZoeDepth][ZoeDepth]]: [[https://arxiv.org/abs/2302.12288][Combining]] relative and metric depth
* ANTI REGULATION - GLOWS
- [[https://arxiv.org/pdf/2303.07345.pdf][erasing]] [[https://github.com/rohitgandikota/erasing][concepts]]
  - [[https://www.reddit.com/r/StableDiffusion/comments/125dli7/using_stable_diffusion_eraser_to_replace_a/][Using stable diffusion]] eraser to replace a concept in one model with the same concept from another
  - [[https://arxiv.org/abs/2303.17591][Forget-Me-Not]]: [[https://github.com/SHI-Labs/Forget-Me-Not][Learning to]] Forget in Text-to-Image Diffusion Models
- [[https://twitter.com/giannis_daras/status/1663710057400524800][Ambient]] Diffusion: train diffusion models given only *corrupted* images as input (copyrightless-ed)
- [[https://twitter.com/_akhaliq/status/1664073210487267335][Tree-Ring Watermarks]]: Fingerprints for Diffusion Images that are Invisible and Robust
  - patterns hiddens in fourier space
- [[https://twitter.com/_akhaliq/status/1669536531298516993][Seeing the World]] through Your Eyes (getting image from reflection of the eyes)

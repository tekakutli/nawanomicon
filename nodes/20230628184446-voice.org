:PROPERTIES:
:ID:       73ac7415-61d5-4266-964a-647a4243ac6c
:END:
#+title: voice
#+filetags: :nawanomicon:
- parent: [[id:e9be16f7-8032-4509-9aa9-7843836eacd9][domain]]
* GENERATION
- [[https://arxiv.org/abs/2303.02939][FoundationTTS]]: Text-to-Speech for ASR Custmization with Generative Language Model (automatic phonems, coerse and fine composition)
- artificial tongue-throat https://twitter.com/ConcreteSciFi/status/1642249097104220160
- google bard
  - [[https://twitter.com/_akhaliq/status/1669736556301631496][Voicebox]]: Text-Guided Multilingual Universal Speech Generation at Scale (20 times faster than valle)
** AUDIO DIFFUSION
- parent: [[id:82127d6a-b3bb-40bf-a912-51fa5134dacc][diffusion]]
- [[https://twitter.com/_akhaliq/status/1648510180009844738][NaturalSpeech 2]]: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers
- music diffusion https://www.arxiv-vanity.com/papers/2301.11757/
* GPT BASED
- https://nonint.com/2022/04/25/tortoise-architectural-design-doc/
  - https://152334h.github.io/blog/tortoise-fine-tuning/
- [[https://arxiv.org/abs/2209.03143][AudioLM]]: [[https://github.com/lucidrains/audiolm-pytorch][a Language]] [[https://google-research.github.io/seanet/audiolm/examples/][Modeling]] Approach to Audio Generation <<gpt voice only>>
  - actually BERT, and using soundstream
  - also tts, and extended to valle, <<AudioLM>>
  - [[https://arxiv.org/abs/2305.09636][SoundStorm]]: Efficient Parallel Audio Generation
    - 2 times faster than AudioLM, 50 fps, 30 seconds of speech continuation within 2 seconds
- https://github.com/suno-ai/bark ==best so far==
- [[https://twitter.com/_akhaliq/status/1666255898749042689][Mega-TTS]]: [[https://mega-tts.github.io/demo-page/][Zero-Shot]] Text-to-Speech at Scale with Intrinsic Inductive Bias
  - decomposed, uses spectrograms, wild-big dataset, phase reconstructed, best zero shot
* AUDIO CODEC
- Disen: [[https://arxiv.org/abs/2211.11960][Disentangled Feature]] Learning for Real-Time Neural Speech Coding
  - voice conversion in real-time communications
  - ==Codec== codebook each for speaker and content
- [[https://github.com/enhuiz/vall-e][valle]] [[https://valle-demo.github.io/][concept]]: modeling (building up decoder)
  - https://vallex-demo.github.io/ [[https://arxiv.org/pdf/2303.03926.pdf][Foreign Languages]] with Your Own
  - [[AudioLM]]
    - [[https://twitter.com/_akhaliq/status/1668430703128707078][High-Fidelity Audio]] [[https://github.com/descriptinc/descript-audio-codec][Compression]] [[https://twitter.com/arankomatsuzaki/status/1668435803373191168][with]] Improved RVQGAN (8kbps)
      - 3 times better than facebook [[https://github.com/facebookresearch/encodec][encodec]] ==best codec==
- [[https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec/][LMCodec]]: A Low Bitrate Speech Codec With Causal Transformer Models
  - [[https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html][Soundstream]] [[https://google-research.github.io/seanet/soundstream/examples/][encoder]] [[https://github.com/wesbz/SoundStream][implementation]]
- [[https://youtu.be/lX0S0ZdWdDw][MusicGen]]: vs google musiclm, inteweaving of discrete sound tokens conditioned on text input
* VOICE CONVERSION
- [[https://arxiv.org/abs/2302.08137][Speech]] [[https://paarthneekhara.github.io/ace/code.html][Representation]] Extractor: divide in parts, voice, pitch, context; zero shot [[https://github.com/NVIDIA/NeMo][Nvidia]]
- [[https://qicongxie.github.io/end2endvc/][end2endvc]]: [[https://arxiv.org/pdf/2206.07569.pdf][End-to-End Voice]] Conversion with Information Perturbation (==better mos than nvc==) (better MOS than freevc)
  - [[https://arxiv.org/pdf/2210.15418.pdf][FREEVC]]: [[https://github.com/olawod/freevc][TOWARDS]] HIGH-QUALITY TEXT-FREE ONE-SHOT VOICE CONVERSION (==vits==)
  - [[https://arxiv.org/pdf/2209.11866.pdf][CONTROLVC]]: ZERO-SHOT VOICE CONVERSION WITH TIME-VARYING CONTROLS ON PITCH AND SPEED
- [[https://arxiv.org/pdf/2302.08296.pdf][QuickVC]] ([[https://github.com/quickvc/QuickVC-VoiceConversion][5000 kHz]] **fastest**) ==vits==
- [[https://arxiv.org/abs/2212.14227][StyleTTS-VC]]: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models (phonems)
- [[https://github.com/anonymous-pits/pits][Pits]]: vits with pitch control (monotomic alignment)
- [[https://arxiv.org/abs/2303.09057][TriAAN-VC]]: [[https://winddori2002.github.io/vc-demo.github.io/][Triple Adaptive]] Attention Normalization for Any-to-Any Voice Conversion
  - **best similarity** and close naturality, speaker encoding
- [[gpt voice only]] (best similarity, semantic tokens)
* STYLE CONVERSION
- [[https://github.com/auspicious3000/SpeechSplit][SpeechSplit]]:  disentangling speech into content, timbre, rhythm and pitch.
  - [[https://github.com/cyhuang-tw/AutoVC][AutoVC]] implementation
    - speaker embeddings: https://github.com/yistLin/dvector
    - [[https://yistlin.github.io/FragmentVC/][FragmentVC]] Timber transfer (better than AutoVC) keeps frecuency
      - [[https://arxiv.org/pdf/2203.16037.pdf][RGSM]]: better than Fragment
- [[https://arxiv.org/pdf/2205.09784.pdf][LVC-VC]]: Voice Conversion with Location-Variable Convolutions
  - simultaneously performing voice conversion while [[https://lvc-vc.github.io/lvc-vc-demo/][generating audio]]
  - smaller than NVC-Net
  - ==has charts==
- [[https://arxiv.org/abs/2106.00992][NVC-Net]]: End-to-End Adversarial Voice Conversion (==SONY==)
  - voice conversion directly on the raw audio waveform
  - ==best one==  3600 kHz fastest
  - https://github.com/sony/ai-research-code  [[https://github.com/sony/ai-research-code/tree/master/nvcnet][nvc]] [[https://nvcnet.github.io/][voices]]
- MFC-StyleVC: [[https://arxiv.org/pdf/2211.08857.pdf][DELIVERING SPEAKING]] [[https://kerwinchao.github.io/lowresourcevc.github.io/][STYLE IN LOW-RESOURCE]] CONVERSION WITH MULTI-FACTOR CONSTRAINTS
  - repeat the utterance; different training objective for **adaptation**, normalizing
  - content, speaker, style on/off
** EMOTION TRANSFER
- [[https://arxiv.org/abs/2302.10536][Nonparallel Emotional]] [[https://demosamplesites.github.io/EVCUP/][Voice Conversion]] For Unseen Speaker-Emotion Pairs Using Dual Domain Adversarial Network & Virtual Domain Pairing (==SONY==)
** USING A DSP
- [[https://www.youtube.com/watch?v=63cXyngKD_s][Audio]] Style Transfer (using a dsp - a daw plugin)
  - gradient estimation instead of having to replace the plugin with a [[https://youtu.be/63cXyngKD_s?t=1235][proxy network]]

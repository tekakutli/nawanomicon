:PROPERTIES:
:ID:       e06c9ae6-abb6-4f82-b951-44ee3a44a1cf
:END:
#+title: clip
#+filetags: :nawanomicon:
- parent: [[id:39d30d24-c374-4d0c-8037-b03ecbf983fa][computer_vision]]
- simo take on better clip papers: https://twitter.com/cloneofsimo/status/1666086583005769728
- [[https://twitter.com/_akhaliq/status/1655395363283431424][COLA]]: How to adapt vision-language models to Compose Objects Localized with Attributes?
  - attributes(adjectives) with its subjects properly identified
* PRIOR ALTERNATIVES
- better clip, nearest neighbor
  - https://arxiv.org/pdf/2110.05208.pdf
    - nearest neighbor, contrastive
  - https://arxiv.org/abs/2111.07783
- Image-and-Language, pixels only no strings: [[https://arxiv.org/abs/2212.08045][CLIPPO]]
  - [[https://arxiv.org/abs/2105.13626][ByT5]], token free, no tokenizer
    - character aware models: [[https://arxiv.org/pdf/2212.10562.pdf][can spell]], like by ByT5
      - maybe hands-aware models?
- [[https://arxiv.org/pdf/2212.00653.pdf][Hyperbolic Contrastive]] [[https://github.com/shlokk/HCL/][Learning]] for Visual Representations beyond Objects
- [[https://twitter.com/_akhaliq/status/1668464076651937792][Retrieval-Enhanced]] Contrastive Vision-Text Models
  - train frozen clip to retrieve knowledge from an external memory
** TEXT MANIPULATION
- [[https://huggingface.co/papers/2305.20088][Improving]] [[https://github.com/LijieFan/LaCLIP][CLIP]] Training with Language Rewrites
  - rewrite the text descriptions associated with each image using an LLM
- [[https://twitter.com/_akhaliq/status/1673518661926264832][Language]] models are weak learners
  - better-than-random performance, boosting component for other models
* CHEAPNESS
- [[https://arxiv.org/abs/2304.06028][RECLIP]]: Resource-efficient CLIP by Training with Small Images
- [[https://twitter.com/_akhaliq/status/1673884289287725057][CLIPA-v2]]: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget
* SCALENESS
- federated clip
  - FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning
  - https://arxiv.org/abs/2302.13485
- [[https://github.com/baaivision/EVA/tree/master/EVA-CLIP][EVA-CLIP]]: [[https://arxiv.org/abs/2303.15389][Improved]] Training Techniques for CLIP at Scale
* FASTNESS
- unum: trained in a day
  - https://github.com/unum-cloud/uform
  - https://www.unum.cloud/blog/2023-02-20-efficient-multimodality
- [[https://twitter.com/_akhaliq/status/1656908423278084096][An Inverse Scaling]] [[https://arxiv.org/abs/2305.07017][Law for]] [[https://github.com/UCSC-VLAA/CLIPA][CLIP Training]], training clip cheaply in 2 days

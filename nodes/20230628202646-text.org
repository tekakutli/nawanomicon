:PROPERTIES:
:ID:       a76fa223-70da-4b76-bf82-1d3ffef3698c
:END:
#+title: text
#+filetags: :nawanomicon:
- [[https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/][OpenSource]] [[https://twitter.com/rskuzma/status/1640721436179308545][Model]] but for new Hardware
- translation prompt: https://boards.4channel.org/g/thread/92468569#p92470651
- [[https://selfrefine.info/][Self-Refine]]: Iterative Refinement with Self-Feedback
- Pix2Struct:
  - DePlot: plot-to-text model helping LLMs understand plots
  - MatCha: great chart & math capabilities by plot deconstruction & numerical reasoning objectives
- [[https://twitter.com/_akhaliq/status/1672046849400909824][From Word Models]] [[https://arxiv.org/pdf/2306.12672.pdf][to World]] [[https://github.com/gabegrand/world-models][Models]]: Translating from Natural Language to the Probabilistic Language of Thought
  - probabilistic programming language = commonsense reasoning, linguistics
- [[https://twitter.com/_akhaliq/status/1665887472335695873][LLM-Blender]]: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion
  - (specialized) text model merging (using rankings)
- [[https://arxiv.org/abs/2303.09014][ART]]: [[https://github.com/bhargaviparanjape/language-programmes/][Automatic]] multi-step reasoning and tool-use for large language models
  - bubbles of logic
- [[https://twitter.com/_akhaliq/status/1676052985544155136][Personality Traits]] in Large Language Models, quantifying personalities
* CORPUS
- [[https://github.com/facebookresearch/NPM][NPM]]: Nonparametric Masked Language Modeling, vs GPTv3, text corpus based
  - other code implementations https://www.catalyzex.com/paper/arxiv:2212.01349/code
- [[https://twitter.com/_akhaliq/status/1680740847128653829][Copy]] Is All You Need
  - task of text generation decomposed into a series of copy-and-paste operations
  - text spans rather than vocabulary
  - learning = text compression algorithm ?
* TRAIN TEXT MODEL
- [[https://arxiv.org/abs/2304.05511][Training]] Large Language Models Efficiently with Sparsity and Dataflow
- [[https://arxiv.org/pdf/2305.11206.pdf][LIMA]]: [[https://twitter.com/_akhaliq/status/1660458199504556034][Less]] Is More for Alignment
  - trained only 1,000 carefully curated prompts and responses
- [[https://huggingface.co/papers/2305.16635][Impossible Distillation]]: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing
  - high-quality model and dataset from a low-quality teacher model
- [[https://huggingface.co/papers/2305.16843][Randomized]] Positional Encodings Boost Length Generalization of Transformers
- [[https://huggingface.co/papers/2305.16958][MixCE]]: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies
  - reverse cross-entropy RATHER THAN maximum likelihood estimation (MLE)
- [[https://huggingface.co/papers/2305.16765][Backpack Language]] Models: non-contextual sense vectors, which specialize encoding different aspects word
- [[https://twitter.com/_akhaliq/status/1670678532349915138][Full Parameter]] Fine-tuning for Large Language Models with Limited Resources, low-memory optimizer
* ADDED - EXTRAS TO LLM
- llama plugins: https://twitter.com/algo_diver/status/1639681733468753925
- llama tools: https://github.com/OpenBMB/ToolBench
- langchain, and https://github.com/srush/MiniChain
  - [[https://arxiv.org/pdf/2305.14564.pdf][PEARL]]: Prompting Large Language Models to Plan and Execute Actions Over Long Documents
* MEMORY
- [[https://arxiv.org/abs/2203.08913][Memorizing]] [[https://twitter.com/nearcyan/status/1637891562385317897][Transformers]] [[https://github.com/google-research/meliad][repo]]
  - Memorizing Transformer does not need to be pre-trained from scratch; possible adding memory to an existing pre-trained model, and then fine-tuning it
- [[https://huggingface.co/papers/2305.16338][Think Before]] You Act: Decision Transformers with Internal Working Memory, task specialized memory
- [[https://twitter.com/_akhaliq/status/1668436285822836737][Augmenting]] Language Models with Long-Term Memory (unlimited context)
* SPECIALIZED
- [[https://arxiv.org/abs/2305.12031][Clinical]] [[https://github.com/bowang-lab/clinical-camel][Camel]]: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding; medical, doctor
- [[https://huggingface.co/papers/2305.19234][Grammar Prompting]] for Domain-Specific Language Generation with Large Language Models
  - predict a BNF grammar given an input, then generates the output according to the rules of that grammar
  - like programming languages
- [[https://twitter.com/_akhaliq/status/1666644201705029632][LLMZip]]: Lossless Text Compression using Large Language Models
- [[https://twitter.com/_akhaliq/status/1666646646103441410][MobileNMT]]: Enabling Translation in 15MB and 30ms
* TEXT DIFFUSION
- parent: [[id:82127d6a-b3bb-40bf-a912-51fa5134dacc][diffusion]]
- [[https://arxiv.org/abs/2212.11685][GENIE]]: Large Scale Pre-training for Text Generation with Diffusion Model
- [[https://arxiv.org/abs/2305.08379][TESS]]: Text-to-Text Self-Conditioned Simplex Diffusion
  - [[https://arxiv.org/abs/2305.09515][AR-Diffusion]]: Auto-Regressive Diffusion Model for Text Generation
- [[https://twitter.com/_akhaliq/status/1665936266372739074][PLANNER]]: Generating Diversified Paragraph via Latent Language Diffusion Model
* LLAMA
- LLaMa ipfs https://www.reddit.com/r/StableDiffusion/comments/11h2wpv/comment/jb59lgt/
  - int-3 quantization: https://nolanoorg.substack.com/p/int-4-llama-is-not-enough-int-3-and [[https://twitter.com/NolanoOrg/status/1635409631530057728][twitter]]
  - llama.cpp [[https://github.com/ggerganov/llama.cpp/pull/301][quantization]]
  - in browser: https://github.com/cocktailpeanut/dalai (there is also the cpp one)
  - [[https://arxiv.org/abs/2303.16199][LLaMA-Adapter]]: [[https://github.com/ZrrSkywalker/LLaMA-Adapter][Efficient Fine-tuning]] of Language Models with Zero-init Attention
    - [[https://arxiv.org/pdf/2302.14691.pdf][In-Context]] [[https://github.com/seonghyeonye/ICIL][Instruction]] Learning (ICIL)
- [[https://arxiv.org/pdf/2304.04947.pdf][Conditional Adapters]]: Parameter-efficient Transfer Learning with Fast Inference
  - [[https://github.com/ZrrSkywalker/LLaMA-Adapter/tree/main/imagebind_LLM][LLaMa-Adapter Multimodal]]! ([[https://twitter.com/lupantech/status/1664316926003396608][vision]])
- [[https://arxiv.org/abs/2304.14318][q2d]]: Turning Questions into Dialogs to Teach Models How to Search
  - our synthetically-generated data achieve 90%--97% of the performance of training on human-generated data
- [[https://github.com/openlm-research/open_llama][Open LLama]], [[https://huggingface.co/openlm-research/open_llama_7b_400bt_preview][Open-Source]] Reproduction, permissively licensed; [[https://github.com/Lightning-AI/lit-llama][Lit-LLaMA]], RedPajama dataset
- [[https://twitter.com/pcuenq/status/1664605575882366980][Falcon]]: new family, open-source ==instruct finetuned too==
- [[https://www.reddit.com/r/LocalLLaMA/comments/13yehfn/new_quantization_method_awq_outperforms_gptq_in/][AWQ]]: Activation-aware Weight Quantization for LLM Compression and Acceleration
  - outperforms GPTQ in 4-bit and 3-bit with 1.45x speedup and works with multimodal LLMs
  - [[https://github.com/Vahe1994/SpQR][SpQR]] [[https://www.reddit.com/r/LocalLLaMA/comments/142ij29/yet_another_quantization_method_spqr_by_tim/][method]] for LLM compression: highly sensitive parameters are not quantized
** FINETUNNING
- finetuning with +loras+ peft https://huggingface.co/blog/trl-peft [[https://twitter.com/younesbelkada/status/1633867640564486144][twitter]] [[https://github.com/huggingface/peft][repo]]
  - Reinforcement Learning with Human Feedback
  - fine-tuning Sentence Transformers: [[https://github.com/huggingface/setfit][SetFit]]
  - [[https://arxiv.org/pdf/2303.09618.pdf][HIVE]]: Harnessing Human Feedback for Instructional Visual Editing (reward model)
  - https://github.com/tloen/alpaca-lora
  - [[https://twitter.com/_akhaliq/status/1661177995049172992][QLoRA]]: Efficient Finetuning of Quantized LLMs, 24 hours 1 gpu 48g
- [[https://huggingface.co/papers/2305.17333][Fine-Tuning Language]] Models with Just Forward Passes, less ram

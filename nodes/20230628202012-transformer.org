:PROPERTIES:
:ID:       d4eebb0c-b7d1-4f56-baf5-004fc69fbd6c
:END:
#+title: transformer
#+filetags: :nawanomicon:
- parent: [[id:cb192d74-71e5-40c3-8763-6f68ffde8e27][train]]
- Star-Transformer: https://arxiv.org/abs/1902.09113
  - [[https://github.com/HazyResearch/safari][Hungry Hungry Hippos]]: State Space Models
    - next: [[https://arxiv.org/pdf/2302.10866.pdf][Hyena Hierarchy]]: gating (cache-d attention) Towards Larger Convolutional Language Models
- contextual transformers (Algorithm Distillation), learns from itself, reinforcement learning
  https://twitter.com/MishaLaskin/status/1585265436723236864
- [[https://arxiv.org/abs/2303.09752][CoLT5]]: Faster Long-Range Transformers with Conditional Computation
  - [[https://twitter.com/papers_daily/status/1637748540653936641][strong gains]] up to 64k input length
- [[https://twitter.com/_akhaliq/status/1664497650702471169][Bytes Are All You Need]]: [[https://huggingface.co/papers/2306.00238][Transformers]] Operating Directly On File Bytes
- [[https://twitter.com/cloneofsimo/status/1664365355266105344][NoPE]]: dont use positional encoding (PE) in Transformer decoders (GPTs)
- [[https://twitter.com/xiaolonw/status/1677003542249484289][Elastic Decision]] Transformer
  - not optimal to use all history states as inputs for decision, instead shorter history
* ALTERNATIVE
** STATE SPACE
:PROPERTIES:
:ID:       bd80ad1d-64de-4445-98e8-0cec31e1ab32
:END:
- [[https://arxiv.org/abs/2212.14052][H3]]: [[https://www.reddit.com/r/MachineLearning/comments/10kdeex/h3_a_new_generative_language_models_that/][hungry hippos]]: state space model instead of transformers
  - [[https://github.com/BlinkDL/RWKV-LM][RWKV]]: RNN instead of transformers
  - [[https://arxiv.org/pdf/2202.07765.pdf][Perceiver]] few latents instead of transformers
* ABOUT ATTENTION
- [[https://medium.com/@b.terryjack/deep-learning-the-transformer-9ae5e9c5a190][What]] is Q,V,K? multihead attention?
- attention free:
  - [[https://arxiv.org/pdf/2111.15588.pdf][SimpleTRON]]: Simple Transformer with O(N) Complexity (no transformer)
    - vs [[https://arxiv.org/abs/2111.11418][Metaformer]] (poolformer, pureformer)
    - maybe not the same: [[https://github.com/ThilinaRajapakse/simpletransformers][the github]]
- SpectFormer: Frequency and Attention is what you need in a Vision Transformer
- [[https://arxiv.org/pdf/1808.03867.pdf][Pervasive]] [[https://github.com/elbayadm/attn2d][Attention]]: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction
  - two-dimensional convolutions to jointly encode the source-target sequences (translation)
* TRANSFORMER VISION
- parent: [[id:39d30d24-c374-4d0c-8037-b03ecbf983fa][computer_vision]]
- DINO: self-suppervised Vision Transformers https://youtu.be/h3ij3F3cPIk
** TOKENS
:PROPERTIES:
:ID:       bb5bc5a8-876c-43ae-8fa0-ea3d6b7da69f
:END:
- [[https://twitter.com/_akhaliq/status/1645278535878049792][SparseFormer]]: Sparse Visual Recognition via Limited Latent Tokens  <<sparseformer>>
  - codebook for videos tokens, not optical flow, 49 tokens
- [[https://arxiv.org/pdf/2303.11114.pdf][SeiT]]: [[https://github.com/naver-ai/seit][Storage-efficient]] Vision Training with Tokens Using 1% of Pixel Storage, <1% of JPEG images
  - Token-based Storage
** HIERARCHICAL DETAILS
- [[https://twitter.com/_akhaliq/status/1645603021248778241][Slide-Transformer]]: Hierarchical Vision Transformer with Local Self-Attention
- [[https://twitter.com/_akhaliq/status/1668459325805699073][FasterViT]]: Fast Vision Transformers with Hierarchical Attention
- [[https://twitter.com/_akhaliq/status/1679344960150151168][Patch]] n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution
